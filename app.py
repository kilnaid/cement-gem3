import streamlit as st
import os
import time
from pinecone import Pinecone
from google import genai
from google.genai import types
from dotenv import load_dotenv

# 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
load_dotenv()

def get_env(key):
    """.envì™€ st.secretsë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ í™˜ê²½ ë³€ìˆ˜ ë¡œë”"""
    if key in st.secrets:
        return st.secrets[key]
    return os.getenv(key)

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(
    page_title="Cement Expert AI (Gemini 3)",
    page_icon="ğŸ—ï¸",
    layout="wide"
)

# 2. ë¡œê·¸ì¸ ì‹œìŠ¤í…œ
if "logged_in" not in st.session_state:
    st.session_state.logged_in = False

def login_page():
    st.markdown(
        """
        <style>
        .stTextInput > div > div > input {text-align: center;}
        </style>
        """, unsafe_allow_html=True
    )
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.title("ğŸ—ï¸ Cement Expert Login")
        st.markdown("### ì‹œë©˜íŠ¸ ê³µì • ê´€ë¦¬ì ì „ìš©")
        st.caption("Powered by Gemini 3 Flash") 
        with st.form("login_form"):
            uid = st.text_input("ID", placeholder="Enter ID")
            upw = st.text_input("Password", type="password", placeholder="Enter Password")
            submit = st.form_submit_button("ë¡œê·¸ì¸", use_container_width=True)

            if submit:
                # [ë³´ì•ˆ] ì‚¬ìš©ì ì •ë³´ í™•ì¸
                if uid == "kilnaid" and upw == "1q2w3e4r":
                    st.session_state.logged_in = True
                    st.success("ì ‘ì† ìŠ¹ì¸! Gemini 3 ì‹œìŠ¤í…œì— ì—°ê²°í•©ë‹ˆë‹¤...")
                    time.sleep(0.5)
                    st.rerun()
                else:
                    st.error("ğŸš« ID ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# 3. ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ (RAG ì‹œìŠ¤í…œ)
def main_app():
    # API í´ë¼ì´ì–¸íŠ¸ ì—°ê²° (ìºì‹±ì„ í†µí•´ ì†ë„ ìµœì í™”)
    @st.cache_resource
    def init_clients():
        try:
            pc = Pinecone(api_key=get_env("PINECONE_API_KEY"))
            idx = pc.Index(get_env("PINECONE_INDEX_NAME"))
            g_client = genai.Client(api_key=get_env("GEMINI_API_KEY"))
            return idx, g_client
        except Exception as e:
            st.error(f"âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            return None, None

    index, client = init_clients()
    if not index or not client:
        st.stop()

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ”§ System Info")
        st.info(f"Connected to: **{get_env('PINECONE_INDEX_NAME')}**")
        st.markdown("---")
        st.markdown("**Model Specs:**")
        st.caption("ğŸ§  LLM: `gemini-3-flash`")
        st.caption("ğŸ§® Embed: `text-embedding-005`")
        st.markdown("---")
        if st.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
            st.session_state.logged_in = False
            st.rerun()

    # ì±„íŒ… UI í—¤ë”
    st.title("ğŸ—ï¸ ì‹œë©˜íŠ¸ ê³µì • ì§€ëŠ¥í˜• ë¹„ì„œ")
    st.caption("ğŸš€ Powered by Gemini 3 Flash & Pinecone Vector Search")

    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ë°˜ê°‘ìŠµë‹ˆë‹¤, ê´€ë¦¬ìë‹˜. Gemini 3 ì—”ì§„ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹œë©˜íŠ¸ ê³µì •ì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”."}
        ]

    # ëŒ€í™” ê¸°ë¡ ì¶œë ¥
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: í‚¬ë¥¸ ì˜¨ë„ê°€ ê¸‰ê²©íˆ ì˜¤ë¥¼ ë•Œ ì¡°ì¹˜ë²•ì€?)"):
        # 1. ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 2. AI ë‹µë³€ ìƒì„±
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            with st.spinner("ğŸ“š ì—…ë¡œë“œ ë¬¸ì„œ ì •ë°€ ê²€ìƒ‰ ì¤‘..."):
                try:
                    # [Step 1] ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜ (ìµœì‹  ëª¨ë¸ text-embedding-005 ì‚¬ìš©)
                    # ì£¼ì˜: Ingest(ì—…ë¡œë“œ)í•  ë•Œ ì‚¬ìš©í•œ ëª¨ë¸ê³¼ ë™ì¼í•´ì•¼ ê²€ìƒ‰ì´ ì˜ ë©ë‹ˆë‹¤.
                    # ë§Œì•½ ê¸°ì¡´ DBê°€ 004ë¡œ ë˜ì–´ ìˆë‹¤ë©´, DBë¥¼ 005ë¡œ ë‹¤ì‹œ ì ì¬í•˜ëŠ” ê²ƒì„ ê°•ë ¥ ê¶Œì¥í•©ë‹ˆë‹¤.
                    emb_res = client.models.embed_content(
                        model="models/text-embedding-005",
                        contents=prompt
                    )
                    query_vector = emb_res.embeddings[0].values

                    # [Step 2] Pinecone ê²€ìƒ‰
                    search_res = index.query(
                        vector=query_vector,
                        top_k=5,
                        include_metadata=True
                    )

                    # [Step 3] ê²€ìƒ‰ëœ ë¬¸ë§¥(Context) ì¡°ë¦½
                    context_text = ""
                    sources = set()
                    for match in search_res['matches']:
                        meta = match['metadata']
                        context_text += f"\n[ì¶œì²˜: {meta.get('source', 'Unknown')} (P.{int(meta.get('page', 0))})]\n{meta.get('text', '')}\n---"
                        sources.add(f"{meta.get('source')} (P.{int(meta.get('page', 0))})")

                    # [Step 4] LLMì—ê²Œ ë‹µë³€ ìš”ì²­ (Gemini 3 Flash ì ìš©)
                    system_prompt = f"""
                    ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì‹œë©˜íŠ¸ ê³µì • ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                    ì•„ë˜ ì œê³µëœ [ê¸°ìˆ  ë¬¸ì„œ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ ê´€ë¦¬ìì˜ ì§ˆë¬¸ì— ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.
                    
                    - Gemini 3ì˜ ë›°ì–´ë‚œ ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ ë³µí•©ì ì¸ ì¸ê³¼ê´€ê³„ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
                    - ìˆ˜ì¹˜ë‚˜ í™”í•™ì‹($CaO$, $C_3S$ ë“±)ì´ ìˆë‹¤ë©´ ì •í™•í•˜ê²Œ ì¸ìš©í•˜ì„¸ìš”.
                    - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ì—…ë¡œë“œ ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ê³ , ì›¹ ê²€ìƒ‰ê³¼ ì¶”ë¡ ì„ í†µí•´ì„œ ë³´ê°•í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
                    
                    [ê¸°ìˆ  ë¬¸ì„œ ë‚´ìš©]:
                    {context_text}
                    """
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
                    response = client.models.generate_content(
                        model="gemini-3-flash",  # ìµœì‹  ëª¨ë¸ëª…
                        contents=[system_prompt, f"ì§ˆë¬¸: {prompt}"],
                        config=types.GenerateContentConfig(temperature=0.1)
                    )
                    
                    full_response = response.text
                    
                    # ì¶œì²˜ í‘œì‹œ ì¶”ê°€
                    if sources:
                        full_response += "\n\n**ğŸ“Œ ì°¸ì¡° ë¬¸ì„œ:**\n- " + "\n- ".join(sorted(list(sources)))

                    message_placeholder.markdown(full_response)
                
                except Exception as e:
                    # ëª¨ë¸ëª… ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆë¹„ì±… ì•ˆë‚´
                    if "404" in str(e) and "gemini-3-flash" in str(e):
                         st.error("âš ï¸ 'gemini-3-flash' ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ ê¶Œí•œì„ í™•ì¸í•˜ê±°ë‚˜ 'gemini-2.0-flash'ë¡œ ë³€ê²½í•´ ë³´ì„¸ìš”.")
                    elif "text-embedding-004" in str(e):
                         st.error("âš ï¸ ì„ë² ë”© ëª¨ë¸ ì—ëŸ¬: 005 ë²„ì „ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ë©´, ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                    else:
                        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
                    full_response = "ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

            # ëŒ€í™” ê¸°ë¡ ì €ì¥
            st.session_state.messages.append({"role": "assistant", "content": full_response})

# ë©”ì¸ ì‹¤í–‰ ë¡œì§
if __name__ == "__main__":
    if not st.session_state.logged_in:
        login_page()
    else:
        main_app()